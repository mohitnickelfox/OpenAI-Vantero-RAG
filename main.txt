### Optimized Fast Code

import os
import random
import json
import csv
import pandas as pd
from datetime import datetime
from typing import List, Dict, Optional
from langchain_openai import OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from pinecone import Pinecone
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from dotenv import load_dotenv
import asyncio
from concurrent.futures import ThreadPoolExecutor
import functools

load_dotenv()

# --- Config ---
PINECONE_INDEX = "restaurant-menu-excel"
SCENARIOS_CSV = "scenarios.csv"
PERSONAS_CSV = "personas.csv"
ADDITIONAL_CONTEXT_CSV = "additional_context.csv"
QUESTIONS_CSV = "Questions.csv"
OPENAI_MODEL = "text-embedding-3-small"

# --- Global Cache ---
_embedding_cache = {}
_feedback_examples_cache = None
_csv_data_cache = {}

# --- Initializations ---
embedder = OpenAIEmbeddings(
    model=OPENAI_MODEL,
    openai_api_key=os.environ.get("OPENAI_API_KEY")
)
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
index = pc.Index(PINECONE_INDEX)

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash-latest",
    google_api_key=os.environ.get("GOOGLE_API_KEY"),
    temperature=0.7,
    max_tokens=1000  # Limit response size for faster generation
)

# --- Optimized Prompt (Shorter) ---
QUESTION_PROMPT = """
You are an expert restaurant trainer bot. Your goal is to generate a high-quality, challenging training question.

Based on past feedback, here are examples of what to do and what to avoid. Learn from them.Focus on 'feedback_notes' column to improve the question and reasoning generation.
{feedback_examples}

Now, using the following NEW details, generate a completely new question that meets the highest standards.

RELEVANT SCENARIO: {scenario}
RELEVANT PERSONA: {persona}
RELEVANT ADDITIONAL CONTEXT: {additional_context}
MENU CONTEXT: {menu_context}

Instructions: Use only the new inputs to craft a question that is both strictly relevant and challenging for waitstaff. The question must require reasoning across all provided details.

Respond ONLY with a valid JSON as follows:
{{
  "question": "...",
  "options": {{
    "A": "...",
    "B": "...",
    "C": "...",
    "D": "..."
  }},
  "correct_answer": "A",
  "reasoning": "...",
  "menu_items_referenced": ["..."]
}}
Requirements:
- Use ONLY menu items and details from the provided context.
- The question must relate directly to the provided scenario, persona, and additional context.
- Create practical, scenario-based questions.
- Include 4 plausible options (A, B, C, D) with only ONE correct answer.
- Reference at least 2 different menu items.
- Ensure the reasoning for the correct answer is STRICTLY brief and explains why the answer is correct.
- The reasoning should be strictly valid , should strictly include respective dish name instead of 'Option A'.
- The options generated for the question should be confusing and hard to guess.
- Keep the question, options, and reasoning SHORT, CRISP, and TO THE POINT in order to fit mobile screen.
"""

# --- Optimized Helper Functions ---

@functools.lru_cache(maxsize=128)
def load_titles_cached(file_path: str) -> tuple:
    """Cached CSV loading with tuple return for hashability"""
    if not os.path.exists(file_path):
        print(f"File not found: {file_path}")
        return tuple()
    
    # Check cache first
    if file_path in _csv_data_cache:
        return _csv_data_cache[file_path]
    
    try:
        df = pd.read_csv(file_path, encoding='utf-8')
    except UnicodeDecodeError:
        df = pd.read_csv(file_path, encoding='cp1252')
    
    df.columns = df.columns.str.strip()
    if 'title' not in df.columns:
        raise Exception(f"CSV {file_path} missing 'title' column.")
    
    titles = tuple(df['title'].dropna().astype(str).tolist())
    _csv_data_cache[file_path] = titles
    return titles

def load_titles(file_path: str) -> List[str]:
    """Convert cached tuple back to list"""
    return list(load_titles_cached(file_path))

def embed_cached(text: str) -> np.ndarray:
    """Cached embedding function"""
    if text in _embedding_cache:
        return _embedding_cache[text]
    
    embedding = np.array(embedder.embed_query(text)).reshape(1, -1)
    _embedding_cache[text] = embedding
    return embedding

def find_relevant_batch(item: str, candidates: List[str]) -> str:
    """Optimized batch similarity calculation"""
    if not candidates:
        return ""
    
    # Use cached embeddings
    item_vec = embed_cached(item)
    
    # Batch embed candidates if not cached
    uncached = [c for c in candidates if c not in _embedding_cache]
    if uncached:
        # Batch embed uncached items
        batch_embeddings = embedder.embed_documents(uncached)
        for i, text in enumerate(uncached):
            _embedding_cache[text] = np.array(batch_embeddings[i]).reshape(1, -1)
    
    # Get all candidate vectors from cache
    c_vecs = np.vstack([_embedding_cache[c] for c in candidates])
    sims = cosine_similarity(item_vec, c_vecs)[0]
    best_idx = np.argmax(sims)
    return candidates[best_idx]

def get_menu_context_fast(query: str, k: int = 3) -> List[Dict]:  # Reduced from 5 to 3
    """Faster menu context retrieval with reduced results"""
    try:
        query_vec = embed_cached(query).flatten()
        response = index.query(
            vector=query_vec.tolist(), 
            top_k=k, 
            include_metadata=True, 
            include_values=False
        )
    except Exception as e:
        print(f"  [ERROR] Pinecone query failed: {e}")
        return []

    docs = []
    for match in response.get("matches", []):
        meta = match.get("metadata", {})
        # Only include most important fields for speed
        key_fields = ["Name", "Food Category", "Menu Description", "Ingredients", "Allergens"]
        parts = [f"{key.upper()}: {meta[key]}" for key in key_fields if meta.get(key)]
        docs.append({"content": "\n".join(parts), "metadata": meta})
    return docs

def extract_json_fast(raw: str) -> Optional[Dict]:
    """Faster JSON extraction"""
    # Find JSON boundaries more efficiently
    start = raw.find('{')
    end = raw.rfind('}')
    if start == -1 or end == -1:
        return None
    
    json_str = raw[start:end+1]
    try:
        return json.loads(json_str)
    except Exception:
        # Try cleaning common issues
        json_str = json_str.replace('\n', ' ').replace('  ', ' ')
        try:
            return json.loads(json_str)
        except Exception:
            return None

def get_feedback_examples_cached() -> str:
    """Cached feedback examples"""
    global _feedback_examples_cache
    
    if _feedback_examples_cache is not None:
        return _feedback_examples_cache
    
    if not os.path.exists(QUESTIONS_CSV):
        _feedback_examples_cache = "No past feedback available."
        return _feedback_examples_cache
    
    try:
        df = pd.read_csv(QUESTIONS_CSV)
        if len(df) < 2:
            _feedback_examples_cache = "Building feedback knowledge..."
            return _feedback_examples_cache

        df['rating'] = pd.to_numeric(df['rating'], errors='coerce')
        df = df.dropna(subset=['rating'])
        df_sorted = df.sort_values(by='rating', ascending=False)
        
        # Only use top and bottom example for speed
        good = df_sorted.iloc[0]
        bad = df_sorted.iloc[-1] if df_sorted.iloc[-1]['rating'] < 3 else None
        
        examples = [f"GOOD (5/5): {good['question'][:100]}... - Clear, relevant, challenging"]
        if bad is not None:
            examples.append(f"AVOID (1/5): {bad['question'][:100]}... - {bad['feedback_notes'][:50]}")
        
        _feedback_examples_cache = "\n".join(examples)
        return _feedback_examples_cache
    except Exception:
        _feedback_examples_cache = "Learning from feedback..."
        return _feedback_examples_cache

def generate_combo_fast(menu_context: str, scenarios: List[str], personas: List[str], additional_contexts: List[str]) -> Dict:
    """Faster combo generation using batch processing"""
    # Pre-select random subsets for speed
    scenario_subset = random.sample(scenarios, min(10, len(scenarios)))
    persona_subset = random.sample(personas, min(10, len(personas)))
    context_subset = random.sample(additional_contexts, min(10, len(additional_contexts)))
    
    relevant_additional = find_relevant_batch(menu_context, context_subset)
    composite = menu_context + " " + relevant_additional
    relevant_scenario = find_relevant_batch(composite, scenario_subset)
    persona_context = menu_context + " " + relevant_scenario
    relevant_persona = find_relevant_batch(persona_context, persona_subset)
    
    return {
        "menu_context": menu_context[:800],  # Truncate for speed
        "scenario": relevant_scenario,
        "persona": relevant_persona, 
        "additional_context": relevant_additional
    }

def save_question_fast(question_data: Dict, combo: Dict, rating: int, feedback_notes: str):
    """Faster CSV writing"""
    header = ["id", "scenario", "persona", "additional_context", "question",
              "option_a", "option_b", "option_c", "option_d", "correct_answer",
              "reasoning", "menu_items_referenced", "created_at", "rating", "feedback_notes"]
    
    file_exists = os.path.exists(QUESTIONS_CSV)
    
    with open(QUESTIONS_CSV, "a", newline='', encoding="utf-8") as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(header)
        
        row_id = f"Q_{datetime.now().strftime('%H%M%S')}_{random.randint(100, 999)}"  # Shorter ID
        q = question_data
        writer.writerow([
            row_id, combo["scenario"], combo["persona"], combo["additional_context"],
            q.get("question", ""), q.get("options", {}).get("A", ""),
            q.get("options", {}).get("B", ""), q.get("options", {}).get("C", ""),
            q.get("options", {}).get("D", ""), q.get("correct_answer", ""),
            q.get("reasoning", ""), ", ".join(q.get("menu_items_referenced", [])),
            datetime.now().strftime('%H:%M:%S'), rating, feedback_notes
        ])
    
    # Clear feedback cache to include new data
    global _feedback_examples_cache
    _feedback_examples_cache = None

def get_user_feedback_fast() -> tuple:
    """Streamlined feedback collection"""
    while True:
        try:
            rating = int(input("Rate 1-5 (5=best): ").strip())
            if 1 <= rating <= 5:
                break
            print("Enter 1-5")
        except ValueError:
            print("Enter a number")
    
    notes = input("Quick feedback: ").strip()
    return rating, notes

# --- Pre-defined menu queries for speed ---
FAST_MENU_QUERIES = [
    # Main Categories
    "appetizers", "starters", "small plates", "main dishes", "entrees", "mains",
    "desserts", "sweets", "beverages", "drinks", "cocktails", "wine", "beer",
    
    # Dietary Restrictions & Preferences
    "gluten free", "dairy free", "vegan options", "vegetarian", "keto", "low carb",
    "nut free", "soy free", "sugar free", "organic", "healthy options", "light meals",
    
    # Protein Types
    "seafood", "fish", "shellfish", "chicken", "poultry", "beef", "pork", "lamb",
    "turkey", "duck", "salmon", "tuna", "shrimp", "crab", "lobster",
    
    # Cooking Methods
    "grilled", "fried", "baked", "roasted", "steamed", "sautéed", "braised",
    "smoked", "barbecue", "pan seared", "slow cooked", "raw", "cured",
    
    # Cuisine Types
    "italian", "mexican", "asian", "american", "french", "mediterranean", 
    "indian", "thai", "chinese", "japanese", "greek", "spanish", "german",
    
    # Dish Types
    "pasta", "salads", "soups", "sandwiches", "burgers", "pizza", "tacos",
    "sushi", "noodles", "rice dishes", "steak", "ribs", "wings", "wraps",
    
    # Ingredients & Flavors
    "cheese", "mushrooms", "spinach", "tomatoes", "avocado", "bacon", "herbs",
    "spicy", "mild", "sweet", "savory", "tangy", "creamy", "crispy", "tender",
    
    # Meal Times
    "breakfast", "brunch", "lunch", "dinner", "late night", "kids menu",
    
    # Special Features
    "chef special", "seasonal", "signature", "house made", "fresh", "local",
    "imported", "premium", "sharing", "family style", "combo", "platter",
    
    # Temperature & Texture
    "hot", "cold", "warm", "chilled", "frozen", "crunchy", "smooth", "chunky",
    
    # Portion Sizes
    "large portions", "small portions", "sharing size", "individual", "half portion",
    
    # Price Points
    "affordable", "premium", "value", "budget friendly", "expensive", "mid range",
    
    # Allergens (for awareness)
    "contains nuts", "contains dairy", "contains eggs", "contains shellfish",
    
    # Service Style
    "takeout", "delivery", "dine in", "catering", "buffet", "à la carte",
    
    # Special Occasions
    "romantic", "family friendly", "business lunch", "celebration", "casual",
    
    # Unique Features
    "modifications", "customizable", "build your own", "unlimited", "all you can eat"
]

# --- Main Execution Loop ---
if __name__ == "__main__":
    print("\n🚀 FAST Restaurant Question Generator 🚀\n")
    
    # Pre-load all data
    print("Loading data...")
    scenarios = load_titles(SCENARIOS_CSV)
    personas = load_titles(PERSONAS_CSV)
    additional_contexts = load_titles(ADDITIONAL_CONTEXT_CSV)
    
    # Pre-cache feedback
    get_feedback_examples_cached()
    
    if not all([scenarios, personas, additional_contexts]):
        print("Error: Missing context CSVs.")
        exit(1)
    
    print(f"Loaded: {len(scenarios)} scenarios, {len(personas)} personas, {len(additional_contexts)} contexts")
    print("Cache warmed up. Ready for fast generation!\n")

    while True:
        start_time = datetime.now()
        
        # Use pre-defined queries for consistency
        menu_query = random.choice(FAST_MENU_QUERIES)
        print(f"Searching: '{menu_query}'...")
        
        menu_docs = get_menu_context_fast(menu_query)
        if not menu_docs:
            print(f"No results for {menu_query}, trying another...")
            continue
            
        menu_context = "\n".join([doc["content"] for doc in menu_docs])
        combo = generate_combo_fast(menu_context, scenarios, personas, additional_contexts)
        
        # Build shorter prompt
        feedback_examples = get_feedback_examples_cached()
        prompt = QUESTION_PROMPT.format(
            feedback_examples=feedback_examples,
            scenario=combo['scenario'][:200],
            persona=combo['persona'][:200], 
            additional_context=combo['additional_context'][:200],
            menu_context=combo['menu_context'][:400]
        )
        
        print("Generating with Gemini...")
        llm_response = llm.invoke(prompt)
        result_json = extract_json_fast(
            llm_response.content if hasattr(llm_response, "content") else str(llm_response)
        )
        
        if not result_json:
            print("Invalid response, retrying...")
            continue
        
        generation_time = (datetime.now() - start_time).total_seconds()
        print(f"\n⚡ Generated in {generation_time:.1f}s")
        print("\nQUESTION:")
        print(json.dumps(result_json, indent=2))
        
        rating, feedback_notes = get_user_feedback_fast()
        save_question_fast(result_json, combo, rating, feedback_notes)
        
        total_time = (datetime.now() - start_time).total_seconds()
        print(f"✅ Complete cycle: {total_time:.1f}s\n")
        
        if input("Continue? (y/n): ").lower() != "y":
            break

    print(f"Done! Cache hits: {len(_embedding_cache)} embeddings cached")